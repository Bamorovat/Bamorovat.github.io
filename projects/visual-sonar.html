<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="Visual Sonar - Mobile Robot Navigation Using Omnidirectional Vision">
    <title>Visual Sonar Project - Mohammad Hossein Bamorovat Abadi</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/styles/visual-sonar.css">
</head>
<body>
    <div class="project-hero">
        <div class="hero-content">
            <h1><i class="fas fa-eye"></i> Visual Sonar</h1>
            <p>Mobile robot navigation using visual sonar via omnidirectional vision system</p>
        </div>
    </div>

    <div class="project-content">
        <a href="../index.html#projects" class="back-button">
            <i class="fas fa-arrow-left"></i> Back to Projects
        </a>

        <div class="card">
            <h2>Project Overview</h2>
            <p>
                The Visual Sonar project represents a revolutionary breakthrough in mobile robot navigation that transforms visual information from omnidirectional cameras into sonar-like depth perception. This biomimetic technology draws inspiration from natural sonar systems found in bats and dolphins, enabling autonomous navigation without expensive laser scanners or complex sensor fusion systems.
            </p>
            
            <h3>Core Innovation: Sonar Vision Algorithm</h3>
            <p>
                At the foundation of this research is a novel sonar vision algorithm that processes omnidirectional images without any prior calibration. The system autonomously detects both static and dynamic obstacles in unknown environments, providing real-time navigation capabilities for mobile robots. This approach eliminates the need for expensive RGB-D cameras or laser-based sensors while maintaining comparable accuracy.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/visual-sonar-vectors.jpg" alt="Visual Sonar Algorithm Concept" class="project-image small">
                <p class="image-caption">Visual Sonar algorithm concept showing omnidirectional vision processing</p>
            </div>
            
            <h3>Multi-Layer Image Processing Architecture</h3>
            <p>
                The Visual Sonar system employs a sophisticated multi-layer image processing architecture designed to transform raw omnidirectional visual data into actionable navigation information. This layered approach ensures robust and reliable processing of complex visual scenes while maintaining real-time performance requirements for mobile robot navigation.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/mage-Processing-Architecture.jpg" alt="Multi-Layer Image Processing Architecture" class="project-image">
                <p class="image-caption">Comprehensive multi-layer image processing architecture for Visual Sonar system</p>
            </div>
            
            <h4>Architecture Layers</h4>
            <div style="margin: 1.5rem 0;">
                <ul style="text-align: left; max-width: 800px; margin: 0 auto;">
                    <li><strong>Input Layer:</strong> Raw omnidirectional image acquisition from 360-degree camera systems</li>
                    <li><strong>Preprocessing Layer:</strong> Image enhancement, noise reduction, and light reflection removal</li>
                    <li><strong>Feature Extraction Layer:</strong> Identification of visual landmarks and obstacle boundaries</li>
                    <li><strong>Sonar Conversion Layer:</strong> Transformation of visual information into sonar-like distance measurements</li>
                    <li><strong>Multi-Agent Processing Layer:</strong> Side Sonar Vision with independent zone analysis</li>
                    <li><strong>Decision Layer:</strong> Path planning and navigation command generation</li>
                    <li><strong>Output Layer:</strong> Real-time control signals for robot movement and mapping</li>
                </ul>
            </div>
            
            <p>
                This multi-layer architecture enables the system to handle complex environmental conditions while maintaining the 120ms processing time requirement. Each layer is optimized for specific tasks, allowing for parallel processing and efficient resource utilization. The modular design also facilitates easy maintenance and future enhancements to individual processing components.
            </p>
            
            <h4>Robot Control Architecture</h4>
            <p>
                The navigation system employs a sophisticated multi-layer architecture where different nodes and modules work simultaneously to ensure robust robot control. The main omnidirectional image undergoes preprocessing before being distributed to two critical processing nodes: the Path Estimate node and the Side Sonar Vision (SSV) node.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/Multi Layers Architecture for Robot Control.png" alt="Multi-Layer Robot Control Architecture" class="project-image">
                <p class="image-caption">Comprehensive multi-layer architecture for robot control showing interconnected nodes and data flow</p>
            </div>
            
            <h4>Node Integration and Data Flow</h4>
            <div style="margin: 1.5rem 0;">
                <p><strong>Path Estimate Node:</strong> Calculates environmental data and produces linear and angular velocity commands based on the final vector angle of sonar vision. The system uses adaptive speed control where closer obstacles result in reduced forward speed and increased turning precision.</p>
                
                <p><strong>Side Sonar Vision (SSV) Node:</strong> Monitors the robot's surroundings in three strategic zones (front, right, left) and sends angle and length data to the navigation node. Each side operates independently, providing comprehensive environmental awareness.</p>
                
                <p><strong>Trajectory Node:</strong> Receives odometry information from motor encoders and generates predetermined paths for the robot. This node creates the baseline navigation plan when no obstacles are detected.</p>
                
                <p><strong>Navigation Node:</strong> Acts as the central decision-making unit, receiving data from all other nodes simultaneously. It intelligently switches between trajectory following and obstacle avoidance based on real-time SSV analysis. When the environment is clear, trajectory data is used; when obstacles are detected, path estimate data takes priority.</p>
            </div>
            
            <h4>Adaptive Control Algorithms</h4>
            <p>
                The system implements sophisticated control algorithms that adjust robot behavior based on environmental conditions. The sensitivity of robot performance depends on parameters set in the SSV node - larger angle and length parameters make the robot maintain greater distances from obstacles, while smaller parameters allow closer navigation. This adaptive approach ensures both safety and efficiency in various operational scenarios.
            </p>
            
            <h3>Advanced Mirror Optimization Research</h3>
            <p>
                Extensive research has been conducted on optimizing omnidirectional vision systems through advanced mirror configurations. Four different mirror types were designed and evaluated: small non-uniform pixel-density hyperbolic mirrors, small uniform pixel density hyperbolic mirrors, large non-uniform pixel density hyperbolic mirrors, and spherical mirrors. The research demonstrates that small uniform pixel density hyperbolic mirrors provide the best performance for vision-based mobile robot navigation.
            </p>
            
            <h4>Light Reflection Processing</h4>
            <p>
                A critical aspect of omnidirectional vision systems is handling light reflections from mirror surfaces, which can significantly impact image quality and navigation accuracy. Advanced image processing techniques have been developed to identify and remove these unwanted light reflections, ensuring cleaner visual data for the sonar vision algorithms. This preprocessing step is essential for maintaining the reliability of obstacle detection and navigation in various lighting conditions.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/Remove-the-light-reflected-from-the-surface.jpg" alt="Light Reflection Removal Process" class="project-image">
                <p class="image-caption">Advanced image processing technique for removing light reflections from omnidirectional mirror surfaces</p>
            </div>
            
            <h3>Side Sonar Vision (SSV) Innovation</h3>
            <p>
                A novel extension called Side Sonar Vision (SSV) divides the surrounding environment into three strategic zones: front, right, and left sides. Each zone is monitored by individual intelligent agents that analyze data separately and produce critical navigation parameters including angle and length measurements. This multi-agent approach enables the robot to achieve up to 98% path tracking accuracy with collision-free navigation, processing data in approximately 120 milliseconds.
            </p>
            
            <h3>Affordable Mapping Solution</h3>
            <p>
                The research extends beyond navigation to include comprehensive mapping capabilities using only a single omnidirectional camera. By combining visual sonar data with robot odometry, the system generates accurate maps suitable for robot navigation. This affordable approach provides mapping quality comparable to established RGB-D camera and laser-based solutions, making advanced robotic mapping accessible to a broader range of applications.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/apping result with laser comparison in red.png" alt="Mapping Results Comparison" class="project-image">
                <p class="image-caption">Mapping results showing comparison with laser-based sensors (highlighted in red)</p>
            </div>
            
            <h3>Key Features</h3>
            <ul>
                <li><strong>Omnidirectional Vision System:</strong> 360-degree environmental perception without calibration</li>
                <li><strong>Advanced Visual Sonar Algorithms:</strong> Real-time distance estimation and obstacle detection</li>
                <li><strong>Multi-Agent Navigation:</strong> Side Sonar Vision with independent zone monitoring</li>
                <li><strong>Affordable Mapping:</strong> Single-camera mapping solution comparable to expensive alternatives</li>
                <li><strong>High Accuracy:</strong> Up to 98% path tracking accuracy with collision-free navigation</li>
                <li><strong>Real-time Processing:</strong> 120ms processing time for efficient live robot operation</li>
            </ul>

            <h3>Technical Specifications</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                <div style="background: var(--section-bg); padding: 1rem; border-radius: 8px;">
                    <strong>Vision System:</strong><br>
                    Omnidirectional camera with 360° field of view
                </div>
                <div style="background: var(--section-bg); padding: 1rem; border-radius: 8px;">
                    <strong>Processing:</strong><br>
                    Real-time visual processing algorithms
                </div>
                <div style="background: var(--section-bg); padding: 1rem; border-radius: 8px;">
                    <strong>Navigation:</strong><br>
                    Advanced path planning and obstacle avoidance
                </div>
                <div style="background: var(--section-bg); padding: 1rem; border-radius: 8px;">
                    <strong>Platform:</strong><br>
                    Mobile robotic platform with integrated sensors
                </div>
            </div>
            
            <h3>Project Gallery</h3>
            <div class="image-grid">
                <div class="image-item">
                    <img src="../assets/images/projects/visual_sonar/ATRVjr_robot.jpg" alt="ATRV Jr Robot Platform">
                    <p class="image-caption">ATRV Jr mobile robot platform used in experiments</p>
                </div>
                <div class="image-item">
                    <img src="../assets/images/projects/visual_sonar/alibration and polynomial odometry fitting d.png" alt="Calibration and Odometry">
                    <p class="image-caption">Calibration and polynomial odometry fitting process</p>
                </div>
            </div>
        </div>

        <div class="card" id="demo-video">
            <h2>Demo Video</h2>
            <p>Watch the Visual Sonar system in action, demonstrating real-time navigation capabilities:</p>
            <div style="margin: 2rem 0; text-align: center;">
                <iframe width="800" height="450" src="https://www.youtube.com/embed/JKRwDcHyVbo?si=RIE-oyffxGRvLpfc" title="Visual Sonar Mobile Robot Navigation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style="max-width: 100%; border-radius: 12px; box-shadow: var(--card-shadow);"></iframe>
            </div>
        </div>

        <div class="card">
            <h2>Related Publications</h2>
            <p>This project has resulted in several peer-reviewed publications showcasing different aspects of the omnidirectional vision system and mobile robot navigation:</p>
            
            <div class="papers-list">
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-mobile-sonar')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Mobile robot navigation using sonar vision algorithm applied to omnidirectional vision</a>
                    <p>Presents a sonar vision algorithm applied to omnidirectional vision that provides autonomous navigation for mobile robots in unknown environments. Uses omnidirectional images without any prior calibration and detects static and dynamic obstacles.</p>
                </div>
                
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-mirrors-effects')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Effects of Mirrors in Mobile Robot Navigation Based on Omnidirectional Vision</a>
                    <p>Comprehensive analysis of four mirror configurations: small non-uniform pixel-density hyperbolic, small uniform pixel density hyperbolic, large non-uniform pixel density hyperbolic, and spherical mirrors. Demonstrates that small uniform pixel density hyperbolic mirrors achieve the best performance in vision-based mobile robot navigation.</p>
                </div>
                
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-side-sonar')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Side Sonar Vision Applied to Omni-directional Images to Navigate Mobile Robots</a>
                    <p>Introduces Side Sonar Vision (SSV) that divides surrounding sonar vision into front, right, and left sides monitored by individual agents. Achieves up to 98% path tracking accuracy without collisions, with 120ms processing time for real-time navigation.</p>
                </div>
                
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-affordable-mapping')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Affordable robot mapping using omnidirectional vision</a>
                    <p>Novel visual mapping method using only a single omnidirectional camera. Combines visual sonar approach with robot odometry to generate maps suitable for navigation. Results indicate performance comparable to established RGB-D camera and laser-based sensor solutions.</p>
                </div>
            </div>
        </div>

        <div class="card" style="text-align: center;">
            <h2>Project Impact</h2>
            <p>
                The Visual Sonar project has made significant contributions to the field of mobile robotics by demonstrating that sophisticated navigation and mapping capabilities can be achieved using minimal hardware requirements. This research has proven that omnidirectional vision systems can replace expensive sensor arrays while maintaining high performance standards, making advanced robotics more accessible and economically viable.
            </p>
            
            <p>
                The developed algorithms and systems have broad applications across multiple domains including service robotics, autonomous vehicles, industrial automation, and cost-sensitive robotic applications. The 98% path tracking accuracy achieved with 120ms processing time demonstrates the practical viability of this technology for real-world deployment.
            </p>
            
            <div style="margin-top: 2rem;">
                <a href="mailto:m.bamorovvat@gmail.com" class="btn btn-primary">
                    <i class="fas fa-envelope"></i> Discuss This Project
                </a>
                <a href="../index.html#projects" class="btn btn-outline" style="margin-left: 1rem;">
                    <i class="fas fa-arrow-left"></i> Back to Projects
                </a>
            </div>
        </div>
    </div>

    <script>
        // Function to navigate to specific publication
        function navigateToPublication(publicationId) {
            // Store the target publication ID for after navigation
            sessionStorage.setItem('targetPublication', publicationId);
            
            // Navigate to the main page with publications section
            window.location.href = '../index.html#publications';
        }
        
        // Smooth scroll for navigation
        document.addEventListener('DOMContentLoaded', function() {
            const backButton = document.querySelector('.back-button');
            if (backButton) {
                backButton.addEventListener('click', function(e) {
                    e.preventDefault();
                    window.location.href = '../index.html#projects';
                });
            }
        });
    </script>
</body>
</html>
