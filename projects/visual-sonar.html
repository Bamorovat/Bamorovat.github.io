<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="Visual Sonar - Mobile Robot Navigation Using Omnidirectional Vision">
    <title>Visual Sonar Project - Mohammad Hossein Bamorovat Abadi</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/styles/visual-sonar.css">
</head>
<body>
    <div class="project-hero">
        <div class="hero-content">
            <h1><i class="fas fa-eye"></i> Visual Sonar</h1>
            <p>Mobile robot navigation using visual sonar via omnidirectional vision system</p>
            
            <div class="hero-demo" style="margin-top: 2rem;">
                <img src="../assets/images/projects/visual_sonar/visual_sonar_demo.gif" alt="Visual Sonar Demo" class="hero-gif">
            </div>
            
            <div class="hero-dataset-links">
                <a href="#github-repository" class="hero-dataset-btn hero-github-btn">
                    <i class="fab fa-github"></i>
                    <span>Source Code</span>
                </a>
                <a href="#related-publications" class="hero-dataset-btn hero-papers-btn">
                    <i class="fas fa-file-alt"></i>
                    <span>Research Papers</span>
                </a>
            </div>
        </div>
    </div>

    <div class="project-content">
        <a href="../index.html#projects" class="back-button">
            <i class="fas fa-arrow-left"></i> Back to Projects
        </a>

        <div class="card">
            <h2>Project Overview</h2>
            <p>
                The Visual Sonar project represents a breakthrough in affordable mobile robot navigation, transforming visual information from omnidirectional cameras into sonar-like depth perception. By mimicking natural sonar systems found in bats and dolphins, this innovative approach enables autonomous navigation without expensive laser scanners or complex sensor arrays.
            </p>
            
            <p>
                This research demonstrates that sophisticated navigation and mapping capabilities can be achieved using minimal hardware - a single omnidirectional camera - while maintaining performance comparable to expensive RGB-D cameras and laser-based systems. The solution makes advanced robotics more accessible and economically viable across various applications.
            </p>
        </div>

        <div class="card">
            <h2>Core Innovation: Visual Sonar Algorithm</h2>
            <p>
                At the heart of this research is a novel sonar vision algorithm that processes omnidirectional images without requiring any prior calibration. The system autonomously detects both static and dynamic obstacles in unknown environments, providing real-time navigation capabilities that rival traditional sensor-based approaches.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/visual-sonar-vectors.jpg" alt="Visual Sonar Algorithm Concept" class="project-image small">
                <p class="image-caption">Visual Sonar algorithm concept: transforming omnidirectional visual data into sonar-like distance measurements</p>
            </div>
            
            <h3>Key Advantages</h3>
            <ul>
                <li><strong>No Calibration Required:</strong> Processes raw omnidirectional images directly</li>
                <li><strong>Cost-Effective:</strong> Eliminates need for expensive laser scanners or RGB-D cameras</li>
                <li><strong>Real-Time Performance:</strong> Processes data in approximately 120 milliseconds</li>
                <li><strong>High Accuracy:</strong> Achieves up to 98% path tracking accuracy with collision-free navigation</li>
                <li><strong>Universal Application:</strong> Works in both known and unknown environments</li>
            </ul>
        </div>

        <div class="card">
            <h2>System Architecture</h2>
            <p>
                The Visual Sonar system employs a sophisticated multi-layer architecture designed for robust real-time processing. The system transforms raw visual data through several specialized layers, each optimized for specific tasks while maintaining parallel processing capabilities.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/mage-Processing-Architecture.jpg" alt="Multi-Layer Image Processing Architecture" class="project-image">
                <p class="image-caption">Multi-layer image processing architecture enabling real-time visual sonar transformation</p>
            </div>
            
            <h3>Processing Layers</h3>
            <div class="architecture-layers">
                <div class="layer-item">
                    <h4><i class="fas fa-camera"></i> Input Layer</h4>
                    <p>Raw omnidirectional image acquisition from 360-degree camera systems</p>
                </div>
                <div class="layer-item">
                    <h4><i class="fas fa-filter"></i> Preprocessing Layer</h4>
                    <p>Image enhancement, noise reduction, and light reflection removal</p>
                </div>
                <div class="layer-item">
                    <h4><i class="fas fa-search"></i> Feature Extraction Layer</h4>
                    <p>Identification of visual landmarks and obstacle boundaries</p>
                </div>
                <div class="layer-item">
                    <h4><i class="fas fa-wave-square"></i> Sonar Conversion Layer</h4>
                    <p>Transformation of visual information into sonar-like distance measurements</p>
                </div>
                <div class="layer-item">
                    <h4><i class="fas fa-users"></i> Multi-Agent Processing</h4>
                    <p>Side Sonar Vision with independent zone analysis</p>
                </div>
                <div class="layer-item">
                    <h4><i class="fas fa-route"></i> Decision Layer</h4>
                    <p>Path planning and navigation command generation</p>
                </div>
                <div class="layer-item">
                    <h4><i class="fas fa-play"></i> Output Layer</h4>
                    <p>Real-time control signals for robot movement and mapping</p>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>Robot Control Architecture</h2>
            <p>
                The navigation system integrates multiple specialized nodes working simultaneously to ensure robust robot control. Each node handles specific aspects of navigation while contributing to the overall decision-making process.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/Multi Layers Architecture for Robot Control.png" alt="Multi-Layer Robot Control Architecture" class="project-image">
                <p class="image-caption">Comprehensive robot control architecture showing node integration and data flow</p>
            </div>
            
            <h3>Navigation Nodes</h3>
            <div class="node-grid">
                <div class="node-item">
                    <h4><i class="fas fa-route"></i> Path Estimate Node</h4>
                    <p>Calculates environmental data and produces velocity commands based on sonar vision analysis. Uses adaptive speed control where closer obstacles result in reduced forward speed and increased turning precision.</p>
                </div>
                
                <div class="node-item">
                    <h4><i class="fas fa-eye"></i> Side Sonar Vision (SSV) Node</h4>
                    <p>Monitors three strategic zones (front, right, left) independently, providing comprehensive environmental awareness with angle and length data for each zone.</p>
                </div>
                
                <div class="node-item">
                    <h4><i class="fas fa-map-marked-alt"></i> Trajectory Node</h4>
                    <p>Receives odometry information from motor encoders and generates predetermined paths. Creates baseline navigation plans when no obstacles are detected.</p>
                </div>
                
                <div class="node-item">
                    <h4><i class="fas fa-brain"></i> Navigation Node</h4>
                    <p>Central decision-making unit that intelligently switches between trajectory following and obstacle avoidance based on real-time environmental analysis.</p>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>Side Sonar Vision (SSV) Innovation</h2>
            <p>
                A key breakthrough in this research is the Side Sonar Vision (SSV) system, which divides the robot's environment into three strategic monitoring zones. Each zone operates with independent intelligent agents that analyze data separately, enabling sophisticated multi-directional awareness.
            </p>
            
            <h3>Multi-Agent Zone Analysis</h3>
            <div class="ssv-features">
                <div class="ssv-item">
                    <h4><i class="fas fa-arrow-up"></i> Front Zone</h4>
                    <p>Primary navigation zone for forward movement and obstacle detection</p>
                </div>
                <div class="ssv-item">
                    <h4><i class="fas fa-arrow-right"></i> Right Zone</h4>
                    <p>Right-side monitoring for corridor navigation and wall following</p>
                </div>
                <div class="ssv-item">
                    <h4><i class="fas fa-arrow-left"></i> Left Zone</h4>
                    <p>Left-side monitoring for comprehensive environmental awareness</p>
                </div>
            </div>
            
            <p>
                The SSV system enables adaptive control where larger angle and length parameters maintain greater distances from obstacles, while smaller parameters allow closer navigation. This flexibility ensures both safety and efficiency across various operational scenarios.
            </p>
        </div>

        <div class="card">
            <h2>Advanced Mirror Optimization</h2>
            <p>
                Extensive research has been conducted on optimizing omnidirectional vision through advanced mirror configurations. Four different mirror types were designed and evaluated to determine optimal performance characteristics.
            </p>
            
            <h3>Mirror Configuration Analysis</h3>
            <div class="mirror-types">
                <div class="mirror-item">
                    <h4>Small Non-Uniform Hyperbolic</h4>
                    <p>Compact design with variable pixel density distribution</p>
                </div>
                <div class="mirror-item">
                    <h4>Small Uniform Hyperbolic</h4>
                    <p>Optimal performance configuration with consistent pixel density</p>
                </div>
                <div class="mirror-item">
                    <h4>Large Non-Uniform Hyperbolic</h4>
                    <p>Extended field of view with variable density mapping</p>
                </div>
                <div class="mirror-item">
                    <h4>Spherical Mirrors</h4>
                    <p>Alternative configuration for specific application requirements</p>
                </div>
            </div>
            
            <p>
                Research findings demonstrate that <strong>small uniform pixel density hyperbolic mirrors</strong> provide the best performance for vision-based mobile robot navigation, offering optimal balance between image quality and processing efficiency.
            </p>
            
            <h3>Light Reflection Processing</h3>
            <p>
                Advanced image processing techniques identify and remove unwanted light reflections from mirror surfaces, ensuring cleaner visual data for sonar algorithms. This preprocessing step is essential for maintaining navigation reliability across varying lighting conditions.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/Remove-the-light-reflected-from-the-surface.jpg" alt="Light Reflection Removal Process" class="project-image">
                <p class="image-caption">Advanced light reflection removal process for enhanced image quality</p>
            </div>
        </div>

        <div class="card">
            <h2>Affordable Mapping Solution</h2>
            <p>
                Beyond navigation, the research extends to comprehensive mapping capabilities using only a single omnidirectional camera. By combining visual sonar data with robot odometry, the system generates accurate maps suitable for robot navigation at a fraction of traditional mapping system costs.
            </p>
            
            <div style="text-align: center; margin: 2rem 0;">
                <img src="../assets/images/projects/visual_sonar/apping result with laser comparison in red.png" alt="Mapping Results Comparison" class="project-image">
                <p class="image-caption">Mapping results comparison: Visual Sonar output vs. laser-based sensors (highlighted in red)</p>
            </div>
            
            <h3>Mapping Advantages</h3>
            <ul>
                <li><strong>Single Camera Solution:</strong> No additional sensors required</li>
                <li><strong>Comparable Accuracy:</strong> Performance matching RGB-D and laser systems</li>
                <li><strong>Cost Effective:</strong> Significant reduction in hardware requirements</li>
                <li><strong>Real-Time Generation:</strong> Maps created during navigation</li>
            </ul>
        </div>

        <div class="card">
            <h2>Technical Specifications & Performance</h2>
            <div class="specs-grid">
                <div class="spec-item">
                    <h4><i class="fas fa-camera"></i> Vision System</h4>
                    <p>Omnidirectional camera with 360Â° field of view, no calibration required</p>
                </div>
                <div class="spec-item">
                    <h4><i class="fas fa-clock"></i> Processing Speed</h4>
                    <p>Real-time processing in approximately 120 milliseconds</p>
                </div>
                <div class="spec-item">
                    <h4><i class="fas fa-bullseye"></i> Navigation Accuracy</h4>
                    <p>Up to 98% path tracking accuracy with collision-free navigation</p>
                </div>
                <div class="spec-item">
                    <h4><i class="fas fa-robot"></i> Platform Compatibility</h4>
                    <p>Compatible with various mobile robotic platforms and sensor configurations</p>
                </div>
            </div>
            
            <h3>Project Gallery</h3>
            <div class="image-grid">
                <div class="image-item">
                    <img src="../assets/images/projects/visual_sonar/ATRVjr_robot.jpg" alt="ATRV Jr Robot Platform" class="clickable-image" onclick="openImageModal(this)">
                    <p class="image-caption">ATRV Jr mobile robot platform used in experimental validation</p>
                </div>
                <div class="image-item">
                    <img src="../assets/images/projects/visual_sonar/alibration and polynomial odometry fitting d.png" alt="Calibration and Odometry" class="clickable-image" onclick="openImageModal(this)">
                    <p class="image-caption">Calibration and polynomial odometry fitting process for enhanced accuracy</p>
                </div>
            </div>
            
            <!-- Image Modal -->
            <div id="imageModal" class="image-modal" onclick="closeImageModal()">
                <div class="modal-content">
                    <span class="close-button" onclick="closeImageModal()">&times;</span>
                    <img id="modalImage" src="" alt="">
                    <div class="modal-caption" id="modalCaption"></div>
                </div>
            </div>
        </div>

        <div class="card" id="demo-video">
            <h2>Demo Video</h2>
            <p>Watch the Visual Sonar system in action, demonstrating real-time navigation capabilities:</p>
            <div style="margin: 2rem 0; text-align: center;">
                <iframe width="800" height="450" src="https://www.youtube.com/embed/JKRwDcHyVbo?si=RIE-oyffxGRvLpfc" title="Visual Sonar Mobile Robot Navigation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style="max-width: 100%; border-radius: 12px; box-shadow: var(--card-shadow);"></iframe>
            </div>
        </div>

        <div class="card" id="related-publications">
            <h2>Related Publications</h2>
            <p>This project has resulted in several peer-reviewed publications showcasing different aspects of the omnidirectional vision system and mobile robot navigation:</p>
            
            <div class="papers-list">
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-mobile-sonar')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Mobile robot navigation using sonar vision algorithm applied to omnidirectional vision</a>
                    <p>Presents a sonar vision algorithm applied to omnidirectional vision that provides autonomous navigation for mobile robots in unknown environments. Uses omnidirectional images without any prior calibration and detects static and dynamic obstacles.</p>
                </div>
                
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-mirrors-effects')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Effects of Mirrors in Mobile Robot Navigation Based on Omnidirectional Vision</a>
                    <p>Comprehensive analysis of four mirror configurations: small non-uniform pixel-density hyperbolic, small uniform pixel density hyperbolic, large non-uniform pixel density hyperbolic, and spherical mirrors. Demonstrates that small uniform pixel density hyperbolic mirrors achieve the best performance in vision-based mobile robot navigation.</p>
                </div>
                
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-side-sonar')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Side Sonar Vision Applied to Omni-directional Images to Navigate Mobile Robots</a>
                    <p>Introduces Side Sonar Vision (SSV) that divides surrounding sonar vision into front, right, and left sides monitored by individual agents. Achieves up to 98% path tracking accuracy without collisions, with 120ms processing time for real-time navigation.</p>
                </div>
                
                <div class="paper-item">
                    <a href="javascript:void(0)" onclick="navigateToPublication('pub-affordable-mapping')" class="paper-title" style="text-decoration: none; color: var(--primary-color); cursor: pointer;">Affordable robot mapping using omnidirectional vision</a>
                    <p>Novel visual mapping method using only a single omnidirectional camera. Combines visual sonar approach with robot odometry to generate maps suitable for navigation. Results indicate performance comparable to established RGB-D camera and laser-based sensor solutions.</p>
                </div>
            </div>
        </div>

        <div class="card" style="text-align: center;" id="github-repository">
            <h2>GitHub Repository & Resources</h2>
            <p>
                Access the complete source code, documentation, and implementation details for the Visual Sonar project. The repository includes ROS packages, algorithms, and experimental configurations used in this research.
            </p>
            
            <div style="margin-top: 1.5rem;">
                <a href="https://github.com/Bamorovat/VisualSonarRos" target="_blank" class="btn btn-secondary">
                    <i class="fab fa-github"></i> View Source Code
                </a>
            </div>
            
            <div style="margin-top: 1.5rem; padding: 1rem; background: var(--card-bg); border-radius: 8px; border: 1px solid var(--border-color);">
                <p style="margin: 0; font-size: 0.9rem; color: var(--text-secondary);">
                    <i class="fas fa-info-circle"></i> 
                    <strong>Repository Features:</strong> Complete ROS implementation, Visual Sonar algorithms, Side Sonar Vision (SSV) components, experimental datasets, and configuration files for omnidirectional camera systems.
                </p>
            </div>
            
            <div style="margin-top: 2rem;">
                <a href="../index.html#projects" class="btn btn-outline">
                    <i class="fas fa-arrow-left"></i> Back to Projects
                </a>
            </div>
        </div>
    </div>

    <script>
        // Function to navigate to specific publication
        function navigateToPublication(publicationId) {
            // Store the target publication ID for after navigation
            sessionStorage.setItem('targetPublication', publicationId);
            
            // Navigate to the main page with publications section
            window.location.href = '../index.html#publications';
        }
        
        // Image modal functionality
        function openImageModal(img) {
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            const modalCaption = document.getElementById('modalCaption');
            
            modal.style.display = 'flex';
            modalImg.src = img.src;
            modalImg.alt = img.alt;
            modalCaption.textContent = img.alt;
            
            // Prevent body scroll when modal is open
            document.body.style.overflow = 'hidden';
        }
        
        function closeImageModal() {
            const modal = document.getElementById('imageModal');
            modal.style.display = 'none';
            
            // Restore body scroll
            document.body.style.overflow = 'auto';
        }
        
        // Close modal with Escape key
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') {
                closeImageModal();
            }
        });
        
        // Smooth scroll for navigation
        document.addEventListener('DOMContentLoaded', function() {
            const backButton = document.querySelector('.back-button');
            if (backButton) {
                backButton.addEventListener('click', function(e) {
                    e.preventDefault();
                    window.location.href = '../index.html#projects';
                });
            }
            
            // Smooth scroll for all hero navigation links
            const heroNavLinks = document.querySelectorAll('a[href^="#"]');
            heroNavLinks.forEach(link => {
                link.addEventListener('click', function(e) {
                    e.preventDefault();
                    const targetId = this.getAttribute('href').substring(1);
                    const targetSection = document.getElementById(targetId);
                    if (targetSection) {
                        targetSection.scrollIntoView({
                            behavior: 'smooth',
                            block: 'start'
                        });
                    }
                });
            });
        });
    </script>
</body>
</html>
